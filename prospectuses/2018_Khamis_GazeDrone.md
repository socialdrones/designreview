# GazeDrone: Mobile Eye-Based Interaction in Public Space Without Augmenting the User

*by Mohamed Khamis, Anna Kienle, Florian Alt, Andreas Bulling, in DroNet '18* - [10.1145/3213526.3213539](https://doi.org/10.1145/3213526.3213539)

### 1) What are the core research questions investigated through empirical studies?

How might we combine a camera-equipped aerial drone with a computational method to detect sidelong glances for spontaneous (calibration-free) gaze-based interaction?

### 2) What empirical methods have been used to study the research questions?

1. An Indoor Live study with 17 participants.

### 3) What kind of drone and/or other apparatus was used?

- A drone
- 86” projected display

### 4) What results have been obtained from the studies?

- Input was significantly more accurate when stationary compared to when moving parallel to the display.
- No evidence of differences in performance among the other movement conditions.
- There are more “incorrect user inputs” in the parallel movement condition.
- Performing selection tasks is easier compared to sliding tasks
- Proposed Scenarios:  audio guide at tourist attractions, worker assistance in large warehouses, market research, disabled assistance, hands-free interaction while doing sports.

### 5) How do the results inform design?

- Interacting while moving parallel to the display is unnatural and demanding.
- Drone should hover with an adjustable camera angle at an altitude below the user’s height, and use small drones (3 x 3 x 2 cm).
- Users should be warned before drones collect data about them.

