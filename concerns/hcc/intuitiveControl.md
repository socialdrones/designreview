# Intuitive Control

To what degree are participants able to intuitively control the drone via the interaction design under investigation, without extensive documentation or explanation? What kinds of interaction designs would be conducive to such intuitive control?

1. [Drone & Me: An Exploration Into Natural Human-Drone Interaction](2015_Cauchard_DroneAndMe.md) by Jessica R. Cauchard, Jane L. E, Kevin Y. Zhai, and James A. Landay in *UbiComp*
- Participants largely preferred to interact with the drone through gestures alone, then with voice, and then with a combination of gesture and voice (in tandem or in sequence). (A small number of prop-based interactions was discarded from analysis.)
- Many participants initially expressed discomfort in talking with a drone, but grew used to it over time.
- Gestures were preferred as they are quick and allow for precise adjustments and continuous control.
- Repeated waving/sweeping was preferred for navigating the drone when far from the body, while smaller motions with body parts as reference were preferred within body range.
- Framing and "invisible camera" gestures were preferred with high agreement for photo-taking, as well as commands involving the word "picture."

2. [Exploring Interaction Modalities for a Selfie Drone](2015_Chen_Selfie.md) by Chien-Fang Chen, Kang-Ping Liu, and Neng-Hao Yu in *SIGGRAPH Asia*
When taking selfies, users will first hold the smartphone or the camera to a proper location and then check the composition on the screen, and then take the selfie.
Taking selfies with a drone should involve two stages: Positioning and framing.

3.[The Naughty Drone: A Qualitative Research on Drone as Companion Device](2016_Kim_Naughty.md) by Hyun Young Kim, Bomyeong Kim, and Jinwoo Kim in *IMCOM*
- The drone was envisioned in various social roles, clustered as "servant," "pet," "friend," and "bully;" which varied (in a decreasing fashion, respectively) on a principal design dimension identified as "perceived controllability." Participants were more appealed to by a drone that has the ability to fulfill multiple roles, rather than one; but higher preference was expressed for roles with higher perceived controllability.
- Participants expressed preference towards an ability to "train" the drone (e.g. as one would a pet), rather than having direct control over it.

4.[How Would You Gesture Navigate a Drone? A User-Centered Approach to Control a Drone](2016_Obaid_Gesture.md) by Mohammad Obaid, Felix Kistler, Gabriele Kasparaviciute, Asım Evren Yantaç, and Morten Fjeld in *Academic Mindtrek*
- Dynamic" gestures were preferred over "static" ones for almost all commands.
- Gestures of a "deictic" nature were often preferred for navigational commands, while "emblematic" and "metaphoric" gestures were preferred for "take picture" and "record video" commands.

- A reasonably high level of agreement exists between people in terms of which gestures feel intutive for issuing various navigational or movement commands to a drone.
- There is more variance between people in terms of which gestures feel intuitive for commands like "take picture" or "record video."

5. [Drone Near Me: Exploring Touch-Based Human-Drone Interaction](2017_Abtahi_DroneNearMe.md) by Parastoo Abtahi, David Y. Zhao, Jane L. E, and James A. Landay in *IMWUT*
- In the safe-to-touch condition, 39% of interactions were touch-based, gesture was the most common modality.
- Participants in the safe-to-touch condition used touch-based interactions significantly more and gestures significantly less. There was no significant difference in the percentage of use for sound.
- In safe-to-touch condition, 58% of participants used touch as a means of interacting with the safe-to-touch drone. 92% of participants in the safe-to-touch condition indicated that they would feel comfortable touching the drone.
The participants in the control condition found interacting with the drone significantly more mentally demanding.


- In safe-to-touch condition, 58% of participants touched the drone and 39% of all interactions were touch-based.
- Interacting with the safe-to-touch drone was reported to be significantly less mentally demanding than the unsafe drone.
Majority of users (83%) reported that they felt safe while interacting with it.

6. [Drone & Wo: Cultural Influences on Human-Drone Interaction Techniques](2017_Ilene_DroneWo.md) by Jane L. E, Ilene L. E, James A. Landay, and Jessica R. Cauchard in *CHI*
- During all parts of the study, Chinese participants tended to combine sound with gesture more than the US participants.
- US participants seemed to switch from primarily using gestures to using sound more (Part 2), while Chinese participants increased use of multi-modal inputs during Part 2 (and thus all modalities).
- It is evident that within cultures, agreement scores tended to be relatively high.
Across cultures, very few of these highly agreed upon interactions match.

- Support should be designed for multi-modal interaction allowing use of slight variation in gestures, and extra contextual words.
- When Chinese participants used multi-modal interaction, the sound would often (75%) align in meaning with the gestural interaction (i.e., palm out, “stop”). For the other 25% of multi-modal interactions, the sound augmented the gesture used to convey the task.
- There were cultural agreement for the tasks “come closer”, “take a picture” and “go sideways”
- There were cultural disagreement for the tasks “stop”,”fly higher”
- It is important to avoid ambiguity. The addition of voice could also aid in clarifying intention.
- Both Chinese and US participants were likely to compare the interactions with the drone to those with people.
- Chinese participants were less likely than US ones to think of the drone as a pet
- Participants in China allowed the drone to be extremely close,
- Chinese participants overall felt interacting with the drone was more physically demanding
- Chinese participants showed relational behaviours like waving or encouraging the drone.

7.  [Communicating Robot Motion Intent with Augmented Reality](2018_Walker_CommunicatingRobotMotionIntent.md) Michael Walker, Hooman Hedayati, Jennifer Lee, and Daniel Szafir in *HRI*

- NavPoints design was rated significantly higher on clarity than the baseline, but there was no significant effects from the other designs.
- NavPoints, Arrow, and Gaze, were ranked as significantly more helpful than Utilities. NavPoints was rated as significantly more helpful than Gaze, with Arrow ranked marginally more helpful than Gaze.

- ARHMD technology can significantly improve user understandings of robot intent and increase objective task efficiency.
